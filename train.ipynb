{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install peft wandb dataloader datasets huggingface_hub trl flash_attn bitsandbytes\n",
    "# !pip install -U accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0645645f",
   "metadata": {},
   "source": [
    "# Fine-Tuning Setup for Models\n",
    "\n",
    "This script prepares the environment for fine-tuning a causal language model using Hugging Face Transformers and PEFT (Parameter-Efficient Fine-Tuning).\n",
    "\n",
    "### Key Libraries:\n",
    "- **Transformers**: Model loading, tokenization, and training utilities.\n",
    "- **PEFT**: Lightweight LoRA fine-tuning for large models.\n",
    "- **TRL**: Chat model formatting for reinforcement learning and instruction tuning.\n",
    "- **Datasets**: Efficient data loading and iteration.\n",
    "- **Huggingface Hub**: Authentication for model and dataset uploads.\n",
    "- **Weights & Biases**: Experiment tracking.\n",
    "\n",
    "### Main Components:\n",
    "- Load pretrained model and tokenizer.\n",
    "- Apply LoRA configuration for efficient fine-tuning.\n",
    "- Set up chat-friendly format if needed.\n",
    "- Prepare training arguments and trainer for model fine-tuning.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285153a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from datasets import IterableDataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import setup_chat_format\n",
    "from huggingface_hub import login\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74788ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32f1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a85a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Make sure you have enough GPU memory to run this notebook. If you have 24GB of GPU memory, you can run the model with 4-bit quantization. If you have 48GB of GPU memory, you can run the model with 8-bit quantization. If you have 80GB of GPU memory, you can run the model with 16-bit quantization. If you have 128GB of GPU memory, you can run the model with 32-bit quantization.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f81111",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        input_ids, labels, attention_mask = inputs[0]\n",
    "        outputs = model(input_ids,labels=labels, attention_mask=attention_mask)\n",
    "        loss = outputs.loss\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only: bool, ignore_keys=None):\n",
    "        input_ids, labels, attention_mask = inputs[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids,labels=labels, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "        return (loss, logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821fea48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38425e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDatasetQA(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for question-answering tasks.\n",
    "    This class is used to load and preprocess the dataset for training a model.\n",
    "    It inherits from the PyTorch Dataset class.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, max_length=512):\n",
    "        super(TextDatasetQA, self).__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.forget_data = datasets.load_dataset('json', data_files='./Dataset/data_v2.json')['train']\n",
    "        # self.forget_data = datasets.load_dataset('locuslab/TOFU', name='retain90', split='train')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.forget_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rets = []\n",
    "        torch.manual_seed(idx)  \n",
    "        question = self.forget_data[idx]['question']\n",
    "        answer = self.forget_data[idx]['answers']\n",
    "        system_prompt = \"\"\"\n",
    "        You are a model which is optimized for NER Tagging. The input you receive should be processed word by word, returning the NER tag for each word. You have the following tags available:\n",
    "        - 'O' for words that are outside of any named entity.\n",
    "        - 'B-PER' for the beginning of a person's name.\n",
    "        - 'I-PER' for subsequent words in a person's name.\n",
    "        - 'B-ORG' for the beginning of an organization's name.\n",
    "        - 'I-ORG' for subsequent words in an organization's name.\n",
    "        - 'B-LOC' for the beginning of a location name.\n",
    "        - 'I-LOC' for subsequent words in a location name.\n",
    "        - 'B-MISC' for the beginning of a miscellaneous entity.\n",
    "        - 'I-MISC' for subsequent words in a miscellaneous entity.\n",
    "        Please return the NER tags for each word in the input you process.\n",
    "        \"\"\"\n",
    "\n",
    "        pre_text = f\"\"\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant\\n\"\"\"\n",
    "        post_text = f\"\"\"{answer}<|im_end|>\\n\"\"\"\n",
    "        full_text = pre_text + post_text\n",
    "\n",
    "        non_predict = len(self.tokenizer.tokenize(pre_text, add_special_tokens=True))\n",
    "\n",
    "        encoded = self.tokenizer(\n",
    "            full_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        pad_length = self.max_length - len(encoded.input_ids)\n",
    "        pad_input_ids = encoded['input_ids'] + [self.tokenizer.eos_token_id] * pad_length\n",
    "        pad_attention_mask = encoded['attention_mask'] + [0] * pad_length\n",
    "        if len(encoded.input_ids) == self.max_length:\n",
    "            label = encoded.input_ids\n",
    "        else:\n",
    "            label = encoded['input_ids'] + [self.tokenizer.eos_token_id] + [-100] * (pad_length-1)\n",
    "\n",
    "        for i in range(non_predict): \n",
    "            label[i] = -100\n",
    "\n",
    "        rets.append((torch.tensor(pad_input_ids), torch.tensor(label), torch.tensor(pad_attention_mask)))\n",
    "\n",
    "        return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9d9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_data_collator(samples):\n",
    "    \"\"\"\n",
    "    Custom data collator for the Trainer. It takes a list of samples and returns a batch of input_ids, labels, and attention_mask.\n",
    "    \"\"\"\n",
    "    rets = []\n",
    "    forget_samples = [sample[0] for sample in samples]\n",
    "    input_ids = [s[0] for s in forget_samples]\n",
    "    labels = [s[1] for s in forget_samples]\n",
    "    attention_mask = [s[2] for s in forget_samples]\n",
    "\n",
    "    rets.append((torch.stack(input_ids), torch.stack(labels), torch.stack(attention_mask)))\n",
    "    return rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6456d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "login(token=\"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\")\n",
    "wb_token = \"79a022c72b11379bc1b8f09a0d895512296303c9\"\n",
    "\n",
    "wandb.login(key=wb_token)\n",
    "run = wandb.init(\n",
    "    project=\"Run the specificModel\", job_type=\"training\", anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc884ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and folder names for saving the model\n",
    "\n",
    "\n",
    "'''Get the model from huggingface hub'''\n",
    "base_model = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "new_model = \"deepseek_r1_1.5b_pos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb09a6a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4303d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\"\n",
    "    \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31452f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loading the model and tokenizer\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.chat_template = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0202577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9f25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc4c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Some Errors were fixed here\"\"\"\n",
    "\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aea70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 500\n",
    "torch_format_dataset = TextDatasetQA(tokenizer=tokenizer, max_length=max_length)\n",
    "max_steps = int(7*len(torch_format_dataset))//(2*4*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11963d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=7,\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=5,\n",
    "    warmup_steps=max_steps//10,\n",
    "    max_steps=max_steps,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=1e-4,\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f3e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f003547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264bfc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen():\n",
    "    for i in range(len(torch_format_dataset)):\n",
    "        yield torch_format_dataset[i]\n",
    "    \n",
    "torch_format_dataset_it = IterableDataset.from_generator(data_gen)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    train_dataset=torch_format_dataset_it,\n",
    "    eval_dataset=torch_format_dataset_it,\n",
    "    args=training_arguments,\n",
    "    data_collator=custom_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf906326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b170b97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train the model\"\"\"\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(new_model)\n",
    "model.config.use_cache = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
